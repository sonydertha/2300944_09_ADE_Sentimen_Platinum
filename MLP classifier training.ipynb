{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e57b0d-a18d-4da4-9310-768181d1d24b",
   "metadata": {},
   "source": [
    "# NeuralNetwork MLP classifier Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da13d1e6-c594-4462-a6c8-31cef5e26379",
   "metadata": {},
   "source": [
    "## import Libary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84b9d7ec-5e6e-430c-bb91-116c48f35312",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ssetiawan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ssetiawan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ssetiawan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.corpus import stopwords\n",
    "import Sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "# from Sastrawi import POSTagger\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import string\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1091051-58ba-4c23-8683-3afaadf66c46",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad58cf81-5550-45e4-b109-950878b5cf7c",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23318112-aeb5-428c-a686-a5afec1fe9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import preprocessed dataset \n",
    "df_ready = pd.read_csv(\"sentiment_analysis_dataset_filtered_tokens.csv\")\n",
    "df_ready['preprocessed_text'] = df_ready['preprocessed_text'].fillna('')\n",
    "\n",
    "# feature extraction\n",
    "# perpare preprocessed and tokenized text data\n",
    "tokenized_texts = df_ready['preprocessed_text']\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the data to obtain TF-IDF features from text\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cd60ac7-c498-4020-92c9-bb11b46e7684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_sent</th>\n",
       "      <th>filtered_tokens</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>['warung', 'ini', 'dimiliki', 'oleh', 'pengusa...</td>\n",
       "      <td>['warung ini dimiliki oleh pengusaha pabrik ta...</td>\n",
       "      <td>['warung', 'dimiliki', 'pengusaha', 'pabrik', ...</td>\n",
       "      <td>warung dimiliki pengusaha pabrik puluhan terke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>['mohon', 'ulama', 'lurus', 'dan', 'k212', 'mm...</td>\n",
       "      <td>['mohon ulama lurus dan k212 mmbri hujjah part...</td>\n",
       "      <td>['mohon', 'ulama', 'lurus', 'k212', 'mmbri', '...</td>\n",
       "      <td>mohon ulama lurus k212 mmbri hujjah partai diw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>['lokasi', 'strategis', 'di', 'jalan', 'sumatr...</td>\n",
       "      <td>['lokasi strategis di jalan sumatra bandung te...</td>\n",
       "      <td>['lokasi', 'strategis', 'jalan', 'sumatra', 'b...</td>\n",
       "      <td>lokasi strategis jalan sumatra bandung nya nya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>positive</td>\n",
       "      <td>['betapa', 'bahagia', 'nya', 'diri', 'ini', 's...</td>\n",
       "      <td>['betapa bahagia nya diri ini saat unboxing pa...</td>\n",
       "      <td>['betapa', 'bahagia', 'nya', 'unboxing', 'pake...</td>\n",
       "      <td>betapa bahagia nya unboxing paket barang nya b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>['aduh', 'jadi', 'mahasiswa', 'jangan', 'sombo...</td>\n",
       "      <td>['aduh jadi mahasiswa jangan sombong dong kasi...</td>\n",
       "      <td>['aduh', 'mahasiswa', 'sombong', 'kasih', 'kar...</td>\n",
       "      <td>aduh mahasiswa sombong kasih kartu kuning bela...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     label                                             tokens  \\\n",
       "0           0  positive  ['warung', 'ini', 'dimiliki', 'oleh', 'pengusa...   \n",
       "1           1   neutral  ['mohon', 'ulama', 'lurus', 'dan', 'k212', 'mm...   \n",
       "2           2  positive  ['lokasi', 'strategis', 'di', 'jalan', 'sumatr...   \n",
       "3           3  positive  ['betapa', 'bahagia', 'nya', 'diri', 'ini', 's...   \n",
       "4           4  negative  ['aduh', 'jadi', 'mahasiswa', 'jangan', 'sombo...   \n",
       "\n",
       "                                         tokens_sent  \\\n",
       "0  ['warung ini dimiliki oleh pengusaha pabrik ta...   \n",
       "1  ['mohon ulama lurus dan k212 mmbri hujjah part...   \n",
       "2  ['lokasi strategis di jalan sumatra bandung te...   \n",
       "3  ['betapa bahagia nya diri ini saat unboxing pa...   \n",
       "4  ['aduh jadi mahasiswa jangan sombong dong kasi...   \n",
       "\n",
       "                                     filtered_tokens  \\\n",
       "0  ['warung', 'dimiliki', 'pengusaha', 'pabrik', ...   \n",
       "1  ['mohon', 'ulama', 'lurus', 'k212', 'mmbri', '...   \n",
       "2  ['lokasi', 'strategis', 'jalan', 'sumatra', 'b...   \n",
       "3  ['betapa', 'bahagia', 'nya', 'unboxing', 'pake...   \n",
       "4  ['aduh', 'mahasiswa', 'sombong', 'kasih', 'kar...   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  warung dimiliki pengusaha pabrik puluhan terke...  \n",
       "1  mohon ulama lurus k212 mmbri hujjah partai diw...  \n",
       "2  lokasi strategis jalan sumatra bandung nya nya...  \n",
       "3  betapa bahagia nya unboxing paket barang nya b...  \n",
       "4  aduh mahasiswa sombong kasih kartu kuning bela...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ready.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db5f004-5163-4647-81c3-8e2f497d21b1",
   "metadata": {},
   "source": [
    "## model splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1420c33-2e2e-4093-ac71-5cd3ced2df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "   tfidf_features, df_ready['label']\n",
    "   , test_size=0.2, random_state=42\n",
    "   )\n",
    "\n",
    "# tackle imbalance \n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1429992d-c77a-4cdb-831d-c6b9482e5f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f541694c-0d8c-44b3-85a1-04fc5562cebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(max_iter=1000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an MLP Classifier\n",
    "mlp_classifier = MLPClassifier(max_iter=1000)\n",
    "\n",
    "# Fit the model on the training data\n",
    "mlp_classifier.fit(X_train_smote, y_train_smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2fc1841-7162-45bc-bdd5-da5f0a96125f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test data\n",
    "predictions_mlp = mlp_classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0ba5e93-a14e-42cc-b236-f0d555c0246b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n",
      "Accuracy: 0.79\n",
      "Precision: 0.80\n",
      "Recall: 0.79\n",
      "F1-Score: 0.79\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy_mlp = accuracy_score(y_test, predictions_mlp)\n",
    "precision_mlp = precision_score(y_test, predictions_mlp, average='weighted')\n",
    "recall_mlp = recall_score(y_test, predictions_mlp, average='weighted')\n",
    "f1_mlp = f1_score(y_test, predictions_mlp, average='weighted')\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy_mlp:.2f}\")\n",
    "print(f\"Precision: {precision_mlp:.2f}\")\n",
    "print(f\"Recall: {recall_mlp:.2f}\")\n",
    "print(f\"F1-Score: {f1_mlp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea7c3a22-41ee-4e42-b117-4cc9f21eac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Save the tuned logistic regression model \n",
    "model_mlp_filename = 'mlp_model.pkl'\n",
    "# with open(model_mlp_filename, 'wb') as model_file:\n",
    "#     joblib.dump(mlp_classifier, model_mlp_filename)\n",
    "\n",
    "# Load the tuned logistic regression model \n",
    "with open(model_mlp_filename, 'rb') as model_file:\n",
    "    loaded_model_mlp = joblib.load(model_mlp_filename)\n",
    "\n",
    "\n",
    "# ## save and load tfidf_vectorizer\n",
    "# # Save the tfidf_vectorizer used for training\n",
    "tfidf_vectorizer_filename = 'tfidf_vectorizer.pkl'\n",
    "# with open(tfidf_vectorizer_filename, 'wb') as vectorizer_file:\n",
    "#    joblib.dump(tfidf_vectorizer, vectorizer_file)\n",
    "\n",
    "# Load the tfidf_vectorizer used for training\n",
    "with open(tfidf_vectorizer_filename, 'rb') as vectorizer_file:\n",
    "   loaded_tfidf_vectorizer = joblib.load(tfidf_vectorizer_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "084ca857-1063-4153-86d6-f7e13256109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def text_processing(text):\n",
    "#    processed_text = text_cleansing(text)\n",
    "#    processed_text = nltk.word_tokenize(processed_text)\n",
    "#    processed_text = filter_stopwords(processed_text)\n",
    "#    processed_text = stemming(processed_text)\n",
    "#    return processed_text\n",
    "\n",
    "\n",
    "def predict_NN (text):\n",
    "   tfidf_vector = loaded_tfidf_vectorizer.transform([text])  \n",
    "   sentiment = loaded_model_mlp.predict(tfidf_vector)\n",
    "   return sentiment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "778cd176-71c6-4e5e-92e5-fbf1aca8e9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texs:waahahah gimana nih\n",
      "sentiment:['negative']\n"
     ]
    }
   ],
   "source": [
    "text = \"waahahah gimana nih\"\n",
    "\n",
    "print (f\"texs:{text}\")\n",
    "print (f\"sentiment:{predict_NN(text)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
